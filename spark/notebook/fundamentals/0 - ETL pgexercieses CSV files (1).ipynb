{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c79fe-654e-4260-8dcb-3c71b9407757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will \n",
    "- learn the concept of ETL\n",
    "- write ETL jobs for CSV files from `pgexercises` https://pgexercises.com/gettingstarted.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5df111-08b1-4884-9c39-8e2ca4e71195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# What's ETL or ELT?\n",
    "\n",
    "ETL stands for Extract, Transform, Load. In the context of Spark, ETL refers to the process of extracting data from various sources, transforming it into a desired format or structure, and loading it into a target system, such as a data warehouse or a data lake.\n",
    "\n",
    "Here's a breakdown of each step in the ETL process:\n",
    "\n",
    "## Extract\n",
    "This step involves extracting data from multiple sources, such as databases, files (CSV, JSON, Parquet), APIs, or streaming data sources. Spark provides connectors and APIs to read data from a wide range of sources, allowing you to extract data in parallel and efficiently handle large datasets.\n",
    "\n",
    "## Transform\n",
    "In the transform step, the extracted data is processed and transformed according to specific business logic or requirements. This may involve cleaning the data, applying calculations or aggregations, performing data enrichment, filtering, joining datasets, or any other data manipulation operations. Spark provides a powerful set of transformation functions and SQL capabilities to perform these operations efficiently in a distributed and scalable manner.\n",
    "\n",
    "## Load\n",
    "Once the data has been transformed, it is loaded into a target system, such as a data warehouse, a data lake, or another storage system. Spark allows you to write the transformed data to various output formats and storage systems, including databases, distributed file systems (like Hadoop Distributed File System or Amazon S3), or columnar formats like Delta Lake or Apache Parquet. The data can be partitioned, sorted, or structured to optimize querying and analysis.\n",
    "\n",
    "Spark's distributed computing capabilities, scalability, and rich ecosystem of libraries make it a popular choice for ETL workflows. It can handle large-scale data processing, perform complex transformations, and efficiently load data into different target systems.\n",
    "\n",
    "By leveraging Spark for ETL, organizations can extract data from diverse sources, apply transformations to ensure data quality and consistency, and load the transformed data into a central repository for further analysis, reporting, or machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407fe3c4-590a-403f-9199-c2d221c20ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Enable DBFS UI\n",
    "\n",
    "- Setting -> Admin Console -> search for dbfs\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jarviscanada/jarvis_data_eng_demo/feature/data/spark/notebook/spark_fundamentals/img/entable_dbfs.jpg\" width=\"700\">\n",
    "\n",
    "- Refresh the page and view DBFS files from UI\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jarviscanada/jarvis_data_eng_demo/feature/data/spark/notebook/spark_fundamentals/img/dbfs%20ui.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ff3413-eea1-4cfb-a0c1-c4ed0e485121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import `pgexercises` CSV files\n",
    "\n",
    "- The pgexercises CSV data files can be found [here](https://github.com/jarviscanada/jarvis_data_eng_demo/tree/feature/data/spark/data/pgexercises).\n",
    "- The pgexercises schema can be found [here](https://pgexercises.com/gettingstarted.html) (for reference purposes).\n",
    "- Upload the `bookings.csv`, `facilities.csv`, and `members.csv` files using Databricks UI (see screenshot)\n",
    "- You can view the imported files from the DBFS UI.\n",
    "\n",
    "![Upload Files](https://raw.githubusercontent.com/jarviscanada/jarvis_data_eng_demo/feature/data/spark/notebook/spark_fundamentals/img/upload%20file.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e85a8b2-6cf3-40af-92d6-74220a9ce9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Interview Questions\n",
    "\n",
    "While completing the rest of the practice, try to answer the following questions:\n",
    "\n",
    "## Concepts\n",
    "- What is ETL? (Hint: Explain each step)\n",
    "ETL, stands for extract, transform and load. It refers to teh process of extracting data from diffret sources, transforming that data in to desired format and then loading that data to a storage faiclity or a data warehouse for further analyisis .  \n",
    "\n",
    "Extract:\n",
    "\n",
    "Transform:\n",
    "\n",
    "Loading:\n",
    "\n",
    "## Databricks\n",
    "- What is Databricks?\n",
    "- What is a Notebook?\n",
    "- What is DBFS?\n",
    "- What is a cluster? \n",
    "- Is Databricks a data lake or a data warehouse?\n",
    "\n",
    "## Managed Table\n",
    "- What is a managed table in Databricks?\n",
    "- Can you explain how to create a managed table in Databricks?\n",
    "- Can you compare a managed table with an RDBMS table? (Hint: Schema on read vs schema on write)\n",
    "- What is the Hive metastore and how does it relate to managed tables in Databricks?\n",
    "- How does a managed table differ from an unmanaged (external) table in Databricks? (Hint: Consider what happens to the data when the table is deleted)\n",
    "- How can you define a schema for a managed table?\n",
    "\n",
    "## Spark\n",
    "`df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location)`\n",
    "- What does the option(\"inferSchema\", \"true\") do? \n",
    "- What does the option(\"header\", \"true\") do?\n",
    "- How can you write data to a managed table?\n",
    "- How can you read data from a managed table into a DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c084e17-00f7-4386-9ca9-4d867b53817d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30083149-8fbc-4a35-a134-dc14ea381a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ETL `bookings.csv` file\n",
    "\n",
    "- **Extract**: Load data from CSV file into a DF\n",
    "- **Transformation**: no transformation needed as we want to load data as it\n",
    "- **Load**: Save the DF into a managed table (or Hive table); \n",
    "\n",
    "# Managed Table\n",
    "This is an important interview topic. Some people may refer to managed tables as Hive tables.\n",
    "\n",
    "https://docs.databricks.com/data-governance/unity-catalog/create-tables.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0762a11d-a040-4b67-ab15-374eef15ed38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- bookid: integer (nullable = true)\n |-- facid: integer (nullable = true)\n |-- memid: integer (nullable = true)\n |-- starttime: timestamp (nullable = true)\n |-- slots: integer (nullable = true)\n\n+------+-----+-----+-------------------+-----+\n|bookid|facid|memid|          starttime|slots|\n+------+-----+-----+-------------------+-----+\n|     0|    3|    1|2012-07-03 11:00:00|    2|\n|     1|    4|    1|2012-07-03 08:00:00|    2|\n|     2|    6|    0|2012-07-03 18:00:00|    2|\n|     3|    7|    1|2012-07-03 19:00:00|    2|\n|     4|    8|    1|2012-07-03 10:00:00|    1|\n+------+-----+-----+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "\n",
    "file_location = \"/FileStore/tables/bookings.csv\"\n",
    "\n",
    "# What does `option(\"header\", \"true\")` and `option(\"inferSchema\", \"true\")` do?\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location)\n",
    "\n",
    "# Why the df schema doesn't match the DDL data type? https://pgexercises.com/gettingstarted.html (hint: `option(\"inferSchema\", \"true\")`)\n",
    "df.printSchema()\n",
    "\n",
    "# Here is the solution to define schema manually\n",
    "# Define schema for the bookings table\n",
    "schema = StructType([\n",
    "    StructField(\"bookid\", IntegerType(), True),\n",
    "    StructField(\"facid\", IntegerType(), True),\n",
    "    StructField(\"memid\", IntegerType(), True),\n",
    "    StructField(\"starttime\", TimestampType(), True),\n",
    "    StructField(\"slots\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Read data from CSV file into DataFrame with predefined schema\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(file_location)\n",
    "\n",
    "# No \n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS bookings\")\n",
    "\n",
    "# Write data from DataFrame into managed table\n",
    "df.write.saveAsTable(\"bookings\")\n",
    "\n",
    "display(df.show(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d925a21-98b7-48e7-8dd8-9d8863826747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Complete ETL Jobs\n",
    "\n",
    "- Complete ETL for `facilities.csv` and `members.csv`\n",
    "- Tips\n",
    "  - The Databricks community version will terminate the cluster after a few hours of inactivity. As a result, all managed tables will be deleted. You will need to rerun this notebook to perform the ETL on all files for the other exercises.\n",
    "  - DBFS data will not be deleted when a custer become inactive/deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdad4ceb-f2f3-4fa9-8be1-260eb2f254ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- facid: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- membercost: decimal(5,2) (nullable = true)\n |-- guestcost: decimal(5,2) (nullable = true)\n |-- initialoutlay: decimal(8,2) (nullable = true)\n |-- monthlymaintenance: decimal(8,2) (nullable = true)\n\n+-----+---------------+----------+---------+-------------+------------------+\n|facid|           name|membercost|guestcost|initialoutlay|monthlymaintenance|\n+-----+---------------+----------+---------+-------------+------------------+\n|    0| Tennis Court 1|      5.00|    25.00|     10000.00|            200.00|\n|    1| Tennis Court 2|      5.00|    25.00|      8000.00|            200.00|\n|    2|Badminton Court|      0.00|    15.50|      4000.00|             50.00|\n|    3|   Table Tennis|      0.00|     5.00|       320.00|             10.00|\n|    4| Massage Room 1|     35.00|    80.00|      4000.00|           3000.00|\n+-----+---------------+----------+---------+-------------+------------------+\nonly showing top 5 rows\n\n+-----+---------------+----------+---------+-------------+------------------+\n|facid|           name|membercost|guestcost|initialoutlay|monthlymaintenance|\n+-----+---------------+----------+---------+-------------+------------------+\n|    0| Tennis Court 1|      5.00|    25.00|     10000.00|            200.00|\n|    1| Tennis Court 2|      5.00|    25.00|      8000.00|            200.00|\n|    2|Badminton Court|      0.00|    15.50|      4000.00|             50.00|\n|    3|   Table Tennis|      0.00|     5.00|       320.00|             10.00|\n|    4| Massage Room 1|     35.00|    80.00|      4000.00|           3000.00|\n+-----+---------------+----------+---------+-------------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Write a ETL job for `facilities.csv`\n",
    "\n",
    "#EXTRACT\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DecimalType\n",
    "\n",
    "facilities_file = \"/FileStore/tables/facilities.csv\"\n",
    "\n",
    "# Define schema for the facilities table\n",
    "facilities_schema = StructType([\n",
    "    StructField(\"facid\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"membercost\", DecimalType(5, 2), True),\n",
    "    StructField(\"guestcost\", DecimalType(5, 2), True),\n",
    "    StructField(\"initialoutlay\", DecimalType(8, 2), True),\n",
    "    StructField(\"monthlymaintenance\", DecimalType(8, 2), True)\n",
    "])\n",
    "\n",
    "facilities_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(facilities_schema).load(facilities_file)\n",
    "\n",
    "facilities_df.printSchema()\n",
    "facilities_df.show(5)  # Display first 5 rows\n",
    "\n",
    "#TRANSFORM\n",
    "## Not needed in this scenario\n",
    "\n",
    "#LOADING\n",
    "\n",
    "# drop table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS facilities\") \n",
    "\n",
    "# write data from dataframe into a managed table\n",
    "facilities_df.write.saveAsTable(\"facilities\")\n",
    "\n",
    "display(facilities_df.show(5))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d372e005-1033-41b0-8742-5995a409d13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------+--------------------+-------+--------------+-------------+----------+\n|memid| surname|firstname|             address|zipcode|     telephone|recommendedby|  joindate|\n+-----+--------+---------+--------------------+-------+--------------+-------------+----------+\n|    0|   GUEST|    GUEST|               GUEST|      0|(000) 000-0000|         null|2012-07-01|\n|    1|   Smith|   Darren|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02|\n|    2|   Smith|    Tracy|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02|\n|    3|  Rownam|      Tim|23 Highway Way, B...|  23423|(844) 693-0723|         null|2012-07-03|\n|    4|Joplette|   Janice|20 Crossing Road,...|    234|(833) 942-4710|            1|2012-07-03|\n+-----+--------+---------+--------------------+-------+--------------+-------------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "members_file = \"/FileStore/tables/members.csv\"\n",
    "\n",
    "# Define schema for the members table\n",
    "members_schema = StructType([\n",
    "    StructField(\"memid\", IntegerType(), True),\n",
    "    StructField(\"surname\", StringType(), True),\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"zipcode\", IntegerType(), True),\n",
    "    StructField(\"telephone\", StringType(), True),\n",
    "    StructField(\"recommendedby\", IntegerType(), True),\n",
    "    StructField(\"joindate\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Read members data\n",
    "members_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(members_schema).load(members_file)\n",
    "\n",
    "# Drop table if exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS members\")\n",
    "\n",
    "# Write data from DataFrame into managed table\n",
    "members_df.write.saveAsTable(\"members\")\n",
    "\n",
    "display(members_df.show(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9aa5dc-ea21-4912-963f-67678b129ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Save your work to Git\n",
    "\n",
    "- Export the notebook to IPYTHON format, `notebook top menu bar -> File -> Export -> iphython`\n",
    "- Upload to your Git repository, `your_repo/spark/notebooks/`\n",
    "- Github can render ipython notebook https://github.com/josephcslater/JupyterExamples/blob/master/Calc_Review.ipynb"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3267011519595935,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "0 - ETL pgexercieses CSV files",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
